{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff31769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#@|-----TESS-cont: The TESS contamination tool-----\n",
    "#@|++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d27fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PRF\n",
    "import sys\n",
    "import warnings\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from astropy import wcs\n",
    "import lightkurve as lk\n",
    "from astropy.io import ascii\n",
    "from matplotlib import patches\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.table import Table\n",
    "from colorsys import hsv_to_rgb\n",
    "from astroquery.mast import Catalogs\n",
    "from configparser import ConfigParser\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colorbar import Colorbar\n",
    "import astropy.visualization as stretching\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "from astropy.coordinates import SkyCoord, Angle\n",
    "from matplotlib.collections import PathCollection\n",
    "from matplotlib.legend_handler import HandlerPathCollection\n",
    "from astropy.visualization.mpl_normalize import ImageNormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|Uncomment this line for the tess-cont.ipynb version\n",
    "#@|#####--Configuration file--#########\n",
    "config_file = 'TOI-4479_S41.ini'\n",
    "#@|####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0babe1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|++++Uncomment these lines for the TESS-cont.py version+++++++\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('config_file')\n",
    "#args = parser.parse_args()\n",
    "#config_file = args.config_file\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "#print(\"\")\n",
    "#print(\"@|---------------------@|\")\n",
    "#print(\"        TESS-cont       \")\n",
    "#print(\"@|---------------------@|\")\n",
    "#print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d00976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|Read the configuration file 'config.ini' into a config_object\n",
    "config_path = 'config/'+config_file\n",
    "config_object = ConfigParser()\n",
    "config_object.read(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3265f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|-----------------------------------------\n",
    "#@|------------Sections---------------------\n",
    "#@|-----------------------------------------\n",
    "\n",
    "MANDATORY = config_object['MANDATORY']\n",
    "try:\n",
    "    OPTIONAL = config_object['OPTIONAL']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    APERTURE = config_object['APERTURE']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    HEATMAP = config_object['HEATMAP']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    DILUTION = config_object['DILUTION']\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|-------------------\n",
    "#@|MANDATORY argument\n",
    "#@|-------------------\n",
    "target = MANDATORY['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab326f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|-------------------\n",
    "#@|OPTIONAL arguments\n",
    "#@|-------------------\n",
    "\n",
    "#@|TESS sector to be analysed\n",
    "try:\n",
    "    sector = OPTIONAL['sector']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#@|target name (for the plots and output folders)\n",
    "try:\n",
    "    target_name = OPTIONAL['target_name']\n",
    "except:\n",
    "    target_name = str(target)\n",
    "    \n",
    "#@|search radius for Gaia DR3 targets\n",
    "try:\n",
    "    search_radius = float(OPTIONAL['search_radius'])\n",
    "except:\n",
    "    search_radius = 200 #arcsec\n",
    "    \n",
    "try:\n",
    "    tpf_or_tesscut = OPTIONAL['tpf_or_tesscut']\n",
    "except:\n",
    "    tpf_or_tesscut = 'tpf'\n",
    "    \n",
    "try:\n",
    "    cutout_size = OPTIONAL['cutout_size']\n",
    "    cutout_size = (int(cutout_size.split(',')[0]), \\\n",
    "                   int(cutout_size.split(',')[0]))\n",
    "except:\n",
    "    cutout_size = (11,11) #@|similar to a tpf\n",
    "    \n",
    "try:\n",
    "    method_prf = OPTIONAL['method_prf']\n",
    "except:\n",
    "    method_prf = 'accurate'\n",
    "    \n",
    "#@|legend location of the heatmap\n",
    "\n",
    "try:\n",
    "    loc_legend = OPTIONAL['loc_legend']\n",
    "except:\n",
    "    loc_legend = 'best'\n",
    "    \n",
    "#@|number of contaminant sources to consider individually\n",
    "#@|(for the pie chart and heatmap)    \n",
    "\n",
    "try:\n",
    "    n_sources = int(OPTIONAL['n_sources'])\n",
    "except:\n",
    "    n_sources = 5\n",
    "    \n",
    "try:\n",
    "    img_fmt = OPTIONAL['img_fmt']\n",
    "except:\n",
    "    img_fmt = 'pdfpng'\n",
    "    \n",
    "try:\n",
    "    save_metrics = OPTIONAL['save_metrics'] == 'True'\n",
    "except:\n",
    "    save_metrics = True\n",
    "    \n",
    "try:\n",
    "    gaia_catalog = OPTIONAL['gaia_catalog']\n",
    "except:\n",
    "    gaia_catalog = 'DR3'\n",
    "    \n",
    "try:\n",
    "    plot_target_name = OPTIONAL['plot_target_name'] == 'True'\n",
    "except:\n",
    "    plot_target_name = False\n",
    "    \n",
    "    \n",
    "#@|-------------------\n",
    "#@|APERTURE arguments\n",
    "#@|-------------------  \n",
    "\n",
    "#@|aperture: pipeline, threshold_target_flux, or threshold_median_flux\n",
    "try:\n",
    "    aperture = APERTURE['aperture']\n",
    "except:\n",
    "    aperture = 'pipeline'\n",
    "    \n",
    "#@|minimum flux coming from the target star in a given pixel (only if aperture: threshold_target_flux )\n",
    "try:\n",
    "    threshold_target = float(APERTURE['threshold_target'])\n",
    "except:\n",
    "    threshold_target = 0.7 #@|(values between 0-1)\n",
    "\n",
    "#@|sigmas above the median tpf flux (only if aperture: threshold_median_flux)\n",
    "try:\n",
    "    threshold_median = float(APERTURE['threshold_median'])\n",
    "except:\n",
    "    threshold_median = 3  #sigma\n",
    "\n",
    "#@|save the aperture in a .csv file.\n",
    "try:\n",
    "    save_aper = APERTURE['save_aper'] == 'True'\n",
    "except:\n",
    "    save_aper = False\n",
    "    \n",
    "#@|-------------------\n",
    "#@|HEATMAP arguments\n",
    "#@|-------------------\n",
    "\n",
    "#@|plot target star \n",
    "try:\n",
    "    plot_target = HEATMAP['plot_target'] == 'True'\n",
    "except:\n",
    "    plot_target = True\n",
    "\n",
    "#@|plot main contaminant sources\n",
    "try:\n",
    "    plot_main_contaminants = HEATMAP['plot_main_contaminants'] == 'True'\n",
    "except:\n",
    "    plot_main_contaminants = True\n",
    "\n",
    "#@|plot all Gaia sources\n",
    "try:\n",
    "    plot_all_gaia = HEATMAP['plot_all_gaia'] == 'True'\n",
    "except:\n",
    "    plot_all_gaia = True\n",
    "    \n",
    "    \n",
    "#@|Overplot the flux ratio from the target star\n",
    "\n",
    "try:\n",
    "    plot_percentages =  HEATMAP['plot_percentages'] == 'True'\n",
    "except:\n",
    "    plot_percentages = True\n",
    "    \n",
    "#@|scale factor for the stars. Disk area scales with flux emission\n",
    "try:\n",
    "    scale_factor = float(HEATMAP['scale_factor'])\n",
    "except:\n",
    "    scale_factor = 4000\n",
    "    \n",
    "#@|-------------------------\n",
    "#@|DILUTION arguments\n",
    "#@|--------------------------\n",
    "\n",
    "#@|observed transit depth of the planet candidate    \n",
    "try:\n",
    "    td = float(DILUTION['td'])\n",
    "    transit_depth_analysis = True\n",
    "except:\n",
    "    transit_depth_analysis = False\n",
    "    \n",
    "#@|is the transit depth already corrected for dilution (e.g. SPOC td)?    \n",
    "try:\n",
    "    dilution_corr = DILUTION['dilution_corr'] == 'True'\n",
    "except:\n",
    "    dilution_corr = True\n",
    "\n",
    "#@|transit depth unit    \n",
    "try:\n",
    "    td_unit = DILUTION['td_unit']  #ppm, ppt, per, frac\n",
    "except:\n",
    "    td_unit = 'frac'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e63fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|############################\n",
    "#@|we create the output folder\n",
    "#@|############################\n",
    "try:\n",
    "    os.mkdir('output/'+target_name)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc03fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|------------------------------------------------------------------------------------------\n",
    "#@|we download the tpf (or tesscut) of the target to build the pixel response function (PRF).\n",
    "#@|------------------------------------------------------------------------------------------\n",
    "\n",
    "#@|This will provide us with the following information:\n",
    "#@|the camera (tpf.camera), and the CCD (tpf.ccd) of the tpf.\n",
    "#@|the columns and rows of reference (tpf.column, tpf.row). Note that the PRF is different in each pixel.\n",
    "#@|the aperture mask of the SPOC pipeline (in case it has one): tpf.pipeline_mask.\n",
    "#@|the shape of the tpf in order to create the PRF with the same dimensions: tpf.shape[1:3].\n",
    "\n",
    "#@|++++++++++++++++++++++++++++++++++++\n",
    "#@|download the Target Pixel File (TPF)\n",
    "#@|++++++++++++++++++++++++++++++++++++\n",
    "if tpf_or_tesscut == 'tpf':\n",
    "    try:\n",
    "        #search_result = lk.search_targetpixelfile('TIC '+str(tic), sector = int(sector))\n",
    "        search_result = lk.search_targetpixelfile(str(target), sector = int(sector))\n",
    "    except NameError: search_result = lk.search_targetpixelfile('TIC '+str(tic))\n",
    "    tpf = search_result.download()\n",
    "    tic = tpf.targetid\n",
    "    if len(search_result) == 0:\n",
    "        try:\n",
    "            print(f'Error: There is not a target pixel file for TIC {tic} (sector {sector}).\\\n",
    "            You can try with a tesscut on the FFIs!')\n",
    "        except NameError: print(f'Error: There is not a target pixel file for TIC {tic}.\\\n",
    "            You can try with a tesscut on the FFIs!')\n",
    "        sys.exit()\n",
    "\n",
    "#@|+++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#@|download a tesscut on the TESS full-frame images (FFIs)\n",
    "#@|+++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "if tpf_or_tesscut == 'tesscut':\n",
    "    try:\n",
    "        search_result = lk.search_tesscut(str(target), sector = int(sector))\n",
    "    except NameError: lk.search_tesscut(str(target))\n",
    "    tpf = search_result.download(cutout_size = cutout_size)\n",
    "    tic = tpf.targetid\n",
    "    if len(search_result) == 0:\n",
    "        try:\n",
    "            print(f'Error: An error has occoured downloading the tesscut of TIC {tic} (sector {sector}).')\n",
    "        except NameError: print(f'Error: An error has occoured downloading the tesscut of TIC {tic}.')\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c11a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|modified from tpfplotter (J.Lillo-Box)\n",
    "#@|https://github.com/jlillo/tpfplotter\n",
    "\n",
    "def get_gaia_data(ra, dec, search_radius=250):\n",
    "    c1 = SkyCoord(ra, dec, frame='icrs', unit='deg')\n",
    "    from astroquery.vizier import Vizier\n",
    "    Vizier.ROW_LIMIT = -1\n",
    "    if gaia_catalog == 'DR3':\n",
    "        gaia_cat, catID = \"I/355/gaiadr3\", gaia_catalog\n",
    "    if gaia_catalog == 'DR2':\n",
    "        gaia_cat, catID = \"I/345/gaia2\", gaia_catalog\n",
    "\n",
    "    result = Vizier.query_region(c1, catalog=[gaia_cat],\n",
    "                                 radius=Angle(search_radius, \"arcsec\"))\n",
    "    try:\n",
    "        result = result[gaia_cat]\n",
    "    except:\n",
    "        print('This target is not in Gaia '+catID)\n",
    "        print('Exiting without finishing...')\n",
    "        sys.exit()\n",
    "\n",
    "    no_targets_found_message = ValueError('Either no sources were found in the query region '\n",
    "                                          'or Vizier is unavailable')\n",
    "    too_few_found_message = ValueError('No sources found closer than 1 arcsec to TPF coordinates')\n",
    "    if result is None:\n",
    "        raise no_targets_found_message\n",
    "    elif len(result) == 0:\n",
    "        raise too_few_found_message\n",
    "        \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9480a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|---------------------------------------------------------------------\n",
    "#@|we build an astroy table with all the Gaia sources within a radius R.\n",
    "#@|---------------------------------------------------------------------\n",
    "print(f'Searching for nearby Gaia {gaia_catalog} stars ...')\n",
    "table = get_gaia_data(tpf.ra, tpf.dec, search_radius = search_radius)\n",
    "table = table[~table['Gmag'].value.mask]  #@|we discard those targets with a 'nan' G magnitude\n",
    "print(f'There are {len(table)} stars surrounding {target_name} within a radius of {search_radius} arcsec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef0393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a4129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|from tpfplotter (J.Lillo-Box)\n",
    "#@|https://github.com/jlillo/tpfplotter\n",
    "def get_dr2_id_from_tic(tic):\n",
    "    # Get the Gaia sources\n",
    "    result = Catalogs.query_object('TIC'+tic, radius=.005, catalog=\"TIC\")\n",
    "    IDs = result['ID'].data.data\n",
    "    k = np.where(IDs == tic)[0][0]\n",
    "    GAIAs = result['GAIA'].data.data\n",
    "    #Gaiamags = result['GAIAmag'].data.data\n",
    "\n",
    "    GAIA_k = GAIAs[k]\n",
    "    #Gaiamag_k = Gaiamags[k]\n",
    "\n",
    "    if GAIA_k == '':\n",
    "        GAIA_k = np.nan\n",
    "    #return GAIA_k, Gaiamag_k\n",
    "    return int(GAIA_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb019955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|---------------------------------------------------------------------\n",
    "#@|we obtain which index in 'table' corresponds to our target (idx_target)\n",
    "#@|---------------------------------------------------------------------\n",
    "if tpf_or_tesscut == 'tesscut':\n",
    "    gaia_dr3_target = get_dr2_id_from_tic(str(tic[4:]))\n",
    "if tpf_or_tesscut == 'tpf':\n",
    "    gaia_dr3_target = get_dr2_id_from_tic(str(tic))\n",
    "idx_target = np.where(table['Source'] == gaia_dr3_target)[0][0]\n",
    "#print(idx_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5b4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adea9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|we create a new column within 'table' called 'flux', in which we estimate, based on their G magnitudes, \n",
    "#@|the flux of each star with respect to the flux of our target: flux = f_star/f_target. \n",
    "#@|to do so, we used the following relation:\n",
    "#@|f_star/f_target = 100**((m_target-m_star)/5)    \n",
    "#@|from http://burro.case.edu/Academics/Astr221/Light/magscale.html\n",
    "\n",
    "flux_col = np.zeros(len(table))\n",
    "for i in range(len(table)):\n",
    "    flux_col[i] = 100**((table['Gmag'][idx_target] - table['Gmag'][i]) / 5)\n",
    "table['flux'] = flux_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a883a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|we get the coordinates (RA, Dec) of the Gaia DR3 nearby targets ('coords'), and we use the wcs to convert \n",
    "#@|them into pixel coordinates inside the tpf (pixel_coords).\n",
    "#@|the pixel coordinates are used to:\n",
    "#@|1) get the colnum and rownum of each source (to get the proper PRF)\n",
    "#@|2) overplot the sources over the PRF plot.\n",
    "print(f'Extracting the Gaia {gaia_catalog} coordinates of the nearby targets \\\n",
    "and converting them into pixel coordinates ... ')\n",
    "coords = []\n",
    "pixel_coords = []\n",
    "\n",
    "#@|proper motion correction \n",
    "\n",
    "if gaia_catalog == 'DR3':\n",
    "    t_reference =  2457389            # 2016-01-01 12:00:00.000 | Lindegren et al. (2021)\n",
    "if gaia_catalog == 'DR2':\n",
    "    t_reference = 2457389 - 182.625   # 2015-07-02 21:00:00.000 | Lindegren et al. (2021)\n",
    "    \n",
    "t_inc = (tpf.time[0].jd - t_reference) / 365  #year\n",
    "\n",
    "for i in tqdm(range(len(table))):\n",
    "    \n",
    "    #coords_ac = SkyCoord(table[i]['RA_ICRS'], table[i]['DE_ICRS'], unit=\"deg\") # OLD --- no pm correction ---\n",
    "    coords_ac = SkyCoord(table[i]['RA_ICRS']+np.nan_to_num(table['pmRA'].value.data[i]) / 3600000 * t_inc,\\\n",
    "                         table[i]['DE_ICRS']+np.nan_to_num(table['pmDE'].value.data[i]) / 3600000 * t_inc,\\\n",
    "                         unit=\"deg\")  # defaults to ICRS frame | includes pm correction\n",
    "    pixel_coords_ac = wcs.utils.skycoord_to_pixel(coords_ac, tpf.wcs, origin = 0, mode='all')\n",
    "    coords.append(coords_ac)\n",
    "    pixel_coords.append(pixel_coords_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f0f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|-------------------------\n",
    "#@|----we build the PRF-----\n",
    "#@|-------------------------\n",
    "\n",
    "resampled = np.zeros(tpf.shape[1:3]) #@|TOTAL PRF. It will contain the flux contributions of all Gaia sources.\n",
    "resampled_list = []  #@|list of PRFs. Each item is the PRF of each Gaia target in 'table'.\n",
    "\n",
    "#@|these are fixed values common to all targets\n",
    "cam = tpf.camera\n",
    "ccd = tpf.ccd\n",
    "sector = tpf.sector\n",
    "\n",
    "#print(f'Building the Point Response Functions (PRF) of each Gaia target ... ({method_prf} method)')\n",
    "\n",
    "if method_prf == 'approximate':\n",
    "    #@|In the approximate method, we estimate the prf in the middle of the TPF ONLY ONCE, and use\n",
    "    #@|the obtained distribution for all targets (assuming that its shape won't change much)\n",
    "    prf = PRF.TESS_PRF(cam,ccd,sector,tpf.column+tpf.shape[2]/2, tpf.row+tpf.shape[1]/2)\n",
    "    print('PRF built in the middle of the TPF (approximate method)')\n",
    "    \n",
    "    #@|we locate the computed PRF in each star's location\n",
    "    print('Resampling for the heatmap plot ...')\n",
    "    for i in tqdm(range(len(table))): \n",
    "        \n",
    "        #@|we resample the PRF into the shape of the original tpf\n",
    "        try:\n",
    "            resampled_ac = prf.locate(pixel_coords[i][0], pixel_coords[i][1], table['flux'][i], tpf.shape[1:3])\n",
    "        except:\n",
    "            #@|this exception occours when there are Gaia sources too far away \n",
    "            resampled_ac = np.zeros(tpf.shape[1:3]) \n",
    "        resampled_list.append(resampled_ac)\n",
    "        resampled = resampled + resampled_ac\n",
    "    \n",
    "if method_prf == 'accurate':\n",
    "    #@|In the accurate method, we estimate the PRF for each individual target in each pixel location\n",
    "    #@|This method is generally slower than the approximate method\n",
    "    colrow_array = np.zeros((len(table), 2))\n",
    "    for i in range(len(table)): \n",
    "    \n",
    "        colnum = int(tpf.column + pixel_coords[i][0])\n",
    "        rownum = int(tpf.row + pixel_coords[i][1])\n",
    "\n",
    "        colrow_array[i] = colnum, rownum\n",
    "        \n",
    "    ###############\n",
    "    \n",
    "    colrow_unique = np.unique(colrow_array, axis = 0)  \n",
    "    prf_array = np.zeros(len(colrow_unique)).astype('object')\n",
    "\n",
    "    print('Building the PRFs in each TESS pixel with nearby Gaia sources ... (this might take a while) ')\n",
    "    for i in tqdm(range(len(prf_array))): \n",
    "        prf_array[i] = PRF.TESS_PRF(cam,ccd,sector, colrow_unique[i][0], colrow_unique[i][1])\n",
    "        \n",
    "    ################\n",
    "    \n",
    "    #@|we locate the computed PRFs in each star's location\n",
    "    print('Resampling for the heatmap plot ...')\n",
    "    for i in tqdm(range(len(table))): \n",
    "        colnum = int(tpf.column + pixel_coords[i][0])\n",
    "        rownum = int(tpf.row + pixel_coords[i][1])\n",
    "\n",
    "        idx = np.where((colrow_unique.T[0] == colnum) & (colrow_unique.T[1] == rownum))[0][0]\n",
    "\n",
    "        #@|we resample the PRF into the shape of the original tpf\n",
    "        try:\n",
    "            resampled_ac = prf_array[idx].locate(pixel_coords[i][0], pixel_coords[i][1], \\\n",
    "                                                 table['flux'][i], tpf.shape[1:3])\n",
    "        except:\n",
    "            #@|this exception occours when there are Gaia sources too far away \n",
    "            resampled_ac = np.zeros(tpf.shape[1:3]) \n",
    "            \n",
    "        resampled_list.append(resampled_ac)\n",
    "        resampled = resampled + resampled_ac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60460bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|-----------------CROWDSAP pixel by pixel--------------------#@|\n",
    "CROWDSAP_pixel_by_pixel = resampled_list[idx_target] / resampled\n",
    "#@|------------------------------------------------------------#@|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352392fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|-----------------------------------------#@|\n",
    "#@|---We select/create the proper apeture---#@|\n",
    "#@|-----------------------------------------#@|\n",
    "\n",
    "if aperture == 'pipeline':\n",
    "    aperture_mask = tpf.pipeline_mask\n",
    "    if len(np.where(aperture_mask==True)[0]) == 0:\n",
    "        print(f'The target TIC {tic} does not have a pipeline aperture in sector {sector}. Please modify your config.ini file so that **aperture: threshold_target_flux** or **aperture: threshold_median_flux** in order to create your own aperture (see the documentaion for details on each aperture creation method).')\n",
    "        sys.exit()\n",
    "        \n",
    "if aperture == 'threshold_median_flux':\n",
    "    aperture_mask = tpf.create_threshold_mask(threshold=threshold_median)\n",
    "    \n",
    "if aperture == 'threshold_target_flux':\n",
    "    aperture_mask = CROWDSAP_pixel_by_pixel > threshold_target  \n",
    "    \n",
    "#@|we save te aperture in a .csv file#@|\n",
    "if save_aper:\n",
    "    if aperture == 'threshold_target_flux':\n",
    "        pd.DataFrame(aperture_mask).to_csv(f'output/{target_name}/{target_name}_S{sector}_aperture_{aperture}_{threshold_target}.csv', \\\n",
    "                                       index = False)\n",
    "    if aperture == 'threshold_median_flux':\n",
    "        pd.DataFrame(aperture_mask).to_csv(f'output/{target_name}/{target_name}_S{sector}_aperture_{aperture}_{threshold_median}.csv', \\\n",
    "                                       index = False)   \n",
    "        \n",
    "    #@|open as (e.g) aperture_mask = np.array(pd.read_csv('TIC_282485660_S12_aperture_threshold_median_flux_3.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b58e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|--------------\n",
    "#@|GET 'FLFRCSAP'\n",
    "#@|---------------\n",
    "\n",
    "#@|'FLFRCSAP' is the flux fraction of the target star inside the photometric aperture, compared to the total flux\n",
    "#@|emited by the target star.'FLFRCSAP'only depends on the target star itself.\n",
    "\n",
    "FLFRCSAP = np.sum(resampled_list[idx_target][aperture_mask])\n",
    "###print(f'The FLFRCSAP of TIC {tic} in Sector {sector} is {FLFRCSAP}.')\n",
    "#@|total flux of the target star outside the apertue\n",
    "#np.sum(resampled_list[idx_target][~tpf.pipeline_mask])\n",
    "\n",
    "\n",
    "#@|--------------\n",
    "#@|GET 'CROWDSAP'\n",
    "#@|---------------\n",
    "\n",
    "#@|'CROWDSAP' is the flux fraction of the target star inside the photometric aperture, compared to the total flux\n",
    "#@|inside the aperture coming from all the sources.'CROWDSAP' depends on the target stars and all nearby sources.\n",
    "\n",
    "CROWDSAP = np.sum(resampled_list[idx_target][aperture_mask]) / np.sum(resampled[aperture_mask])\n",
    "###print(f'The CROWDSAP of TIC {tic} in Sector {sector} is {CROWDSAP}.')\n",
    "\n",
    "\n",
    "if save_metrics == True:    \n",
    "\n",
    "    f = open(\"metrics.dat\", \"a+\") \n",
    "    with open(\"metrics.dat\", \"r\") as f:\n",
    "        f_read = f.read()\n",
    "        \n",
    "    with open(\"metrics.dat\", \"a+\") as f:\n",
    "\n",
    "        if 'config_file' in f_read:\n",
    "            pass\n",
    "        if'config_file' not in f_read:\n",
    "            f.write('config_file,CROWDSAP,FLFRCSAP \\n')\n",
    "            \n",
    "        if config_file in f_read:\n",
    "            ###print(f'The CROWDSAP and FLFRCSAP metrics were already computed for {config_file}')\n",
    "            pass\n",
    "        if config_file not in f_read:\n",
    "            f.write(f'{config_file},{CROWDSAP},{FLFRCSAP}'+\"\\n\")\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|Which targets have the main flux contribution to the aperture? \n",
    "#@|we compute the CROWDSAPs of all targtes, and select the highest ones\n",
    "CROWDSAP_list = []\n",
    "for i in range(len(table)):\n",
    "    \n",
    "    CROWDSAP_ac = np.sum(resampled_list[i][aperture_mask]) / np.sum(resampled[aperture_mask])\n",
    "    CROWDSAP_list.append(CROWDSAP_ac)\n",
    "    \n",
    "CROWDSAP_arr = np.array(CROWDSAP_list)\n",
    "    \n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fce967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|we select the targets that contaminate the aperture ('idxs_contam')\n",
    "#@|that is, 'idxs_contam' contains all the indexes except that of the target star\n",
    "idxs_contam = np.where(CROWDSAP_arr!=CROWDSAP)[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|we compute the contamination ratio of the contaminant sources, with respect to the overall contamination.\n",
    "relative_contam = CROWDSAP_arr[idxs_contam] / np.sum(CROWDSAP_arr[idxs_contam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bffa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|we sort the contaminant sources, from more to less contaminant\n",
    "relative_contam_sorted = sorted(relative_contam, reverse = True)\n",
    "#contaminant_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d92a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_sources == 0:\n",
    "    raise Exception(\"Sorry! We are studying flux contamination, so the number of contaminant souces n_sources cannot be 0.\") \n",
    "if n_sources < 0:\n",
    "    raise Exception('Sorry! The number of contaminant sources must be a positive number.')\n",
    "if type(n_sources) != int:\n",
    "    raise Exception('Sorry! The number of contaminant sources must be a positive integer.')\n",
    "relative_contam = relative_contam_sorted[:n_sources]\n",
    "relative_contam_rest = relative_contam_sorted[n_sources:]\n",
    "relative_contam.extend([np.sum(relative_contam_rest)]) #@|we include the remaining contamination from 'Other' stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|we get the indexes of the n_sources more contaminant sources (on 'table')\n",
    "#@|Note: np.argsot does not have an argument 'reverse', so the highest PDCSAPS are at the end of the array\n",
    "\n",
    "idxs_crowdsap_sorted = np.argsort(CROWDSAP_arr)\n",
    "crowdsap_sorted = np.sort(CROWDSAP_arr)\n",
    "idxs_without_CROWDSAP = np.where(crowdsap_sorted != CROWDSAP)\n",
    "\n",
    "idxs_crowdsap_sorted = idxs_crowdsap_sorted[idxs_without_CROWDSAP]\n",
    "crowdsap_sorted  = crowdsap_sorted[idxs_without_CROWDSAP]\n",
    "\n",
    "#@|############################################################\n",
    "idxs_selected_contaminant_sources = idxs_crowdsap_sorted[-n_sources:]\n",
    "#@|###########################################################\n",
    "#CROWDSAP_arr[idxs_selected_contaminant_sources] / np.sum(CROWDSAP_arr[idxs_contam]) #@|just a check we are doing\n",
    "#@|things right.\n",
    "\n",
    "idxs_selected_contaminant_sources = idxs_selected_contaminant_sources[::-1] #@|we reverse it so that the first \n",
    "#@|idexes correspond to the most contaminant sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c533c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|we create the automatic label for the pie chart\n",
    "#@|criterion. Star#1 is the most contaminant within the aperture, Star#2 the second most contaminant, etc.\n",
    "bar_labels = []\n",
    "gaia_names_selected = []\n",
    "pixel_coords_selected = []\n",
    "for i,index in enumerate(idxs_selected_contaminant_sources):\n",
    "    \n",
    "    bar_labels.append(f'Star {i+1}')      #@|this is the good one\n",
    "    #@|gaia names\n",
    "    gaia_names_selected.append(table['Source'][index])\n",
    "    #@|coordinates\n",
    "    pixel_coords_selected.append(pixel_coords[index])\n",
    "\n",
    "bar_labels.append('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|This is to generate fancy color palettes\n",
    "#@|from https://stackoverflow.com/questions/876853/generating-color-ranges-in-python\n",
    "#from colorsys import hsv_to_rgb  (moved up)\n",
    "def get_hex_color_list(num_colors=5, saturation=0.4, value=1.0):\n",
    "    hex_colors = []\n",
    "    hsv_colors = [[float(x / num_colors), saturation, value] for x in range(num_colors)]\n",
    "    \n",
    "    for hsv in hsv_colors:\n",
    "        hsv = [int(x * 255) for x in hsv_to_rgb(*hsv)]\n",
    "    \n",
    "        #Formatted as hexadecimal string using the ':02x' format specifier\n",
    "        hex_colors.append(f\"#{hsv[0]:02x}{hsv[1]:02x}{hsv[2]:02x}\")\n",
    "    \n",
    "    return hex_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7bfe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|################\n",
    "#@|+++Pie chart++++\n",
    "#@|################\n",
    "\n",
    "print(f'Generating the pie chart plot of {target_name} (Sector {sector}) ...')\n",
    "\n",
    "# make figure and assign axis objects\n",
    "#fig, (ax1, ax2) = plt.subplots(1, 2,  width_ratios=[3, 1], figsize=(9, 5))\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1,6]}, figsize=(6.93, 5.5))\n",
    "fig.subplots_adjust(top = 1.41, bottom = 0, right = 1, left = 0, \n",
    "        hspace = 0, wspace = 0)\n",
    "\n",
    "\n",
    "#@|------PIE CHART-----#@|\n",
    "\n",
    "# pie chart parameters\n",
    "pie_ratios = [1-CROWDSAP, CROWDSAP]\n",
    "#pie_labels = ['Nearby', 'Target']     \n",
    "pie_labels = ['', '           ']  #this\n",
    "pie_colors = ['lightgrey','#25BDB0']\n",
    "explode = [0.5, 0]\n",
    "# rotate so that first wedge is split by the x-axis\n",
    "angle = -180 * pie_ratios[0]\n",
    "#wedges, *_ = ax1.pie(pie_ratios, autopct='%1.1f%%', textprops = {'size': '21'}, startangle=angle,\n",
    "                     #labels=pie_labels, explode=explode, colors = pie_colors, radius = 6)\n",
    "wedges, *_ = ax1.pie(pie_ratios, startangle=angle, explode=explode, \\\n",
    "                     colors = pie_colors, radius = 6, labels = pie_labels)\n",
    "\n",
    "#@|-----------------------Pie chart percentages inside the pie------------------------\n",
    "#@|+++Nearby stars+++\n",
    "if pie_ratios[0] >= 0.9995:\n",
    "    ax1.text(1.60, -0.2, f'Nearby ({\"{:1.2f}\".format(pie_ratios[0]*100)}%)', fontsize = 17)\n",
    "if 0.000995 < pie_ratios[0] < 0.0995:\n",
    "    ax1.text(1.20, -0.2, f'Nearby ({\"{:1.1f}\".format(pie_ratios[0]*100)}%)', fontsize = 17)\n",
    "if pie_ratios[0] < 0.000995:\n",
    "    ax1.text(0.80, -0.2, f'Nearby ({\"{:1.2f}\".format(pie_ratios[0]*100)}%)', fontsize = 17)    \n",
    "if 0.0995 < pie_ratios[0] < 0.9995:\n",
    "    ax1.text(1.45, -0.2, f'Nearby ({\"{:1.1f}\".format(pie_ratios[0]*100)}%)', fontsize = 17)\n",
    "#@|+++Target star+++\n",
    "if pie_ratios[1] >= 0.9995:\n",
    "    ax1.text(-5.2, -0.2, f'Target ({\"{:1.2f}\".format(pie_ratios[1]*100)}%)', fontsize = 17)\n",
    "if 0.000995 < pie_ratios[1] < 0.0995:\n",
    "    ax1.text(-4.9, -0.2, f'Target ({\"{:1.1f}\".format(pie_ratios[1]*100)}%)', fontsize = 17)\n",
    "if pie_ratios[1] < 0.000995:\n",
    "    ax1.text(-5.125, -0.2, f'Target ({\"{:1.2f}\".format(pie_ratios[1]*100)}%)', fontsize = 17)    \n",
    "if 0.0995 < pie_ratios[1] < 0.9995:\n",
    "    ax1.text(-5.125, -0.2, f'Target ({\"{:1.1f}\".format(pie_ratios[1]*100)}%)', fontsize = 17)\n",
    "#@|------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#@|------BAR CHART-----#@|\n",
    "\n",
    "# bar chart parameters\n",
    "bar_ratios = relative_contam\n",
    "bar_labels = bar_labels\n",
    "bar_colors = get_hex_color_list(num_colors = n_sources+1, saturation = 0.5, value = 1.0)\n",
    "bottom = 1\n",
    "width = 0.25\n",
    "\n",
    "# Adding from the top matches the legend.\n",
    "labels_ordered = []\n",
    "for j, (height, label) in enumerate(sorted([*zip(bar_ratios, bar_labels)])):\n",
    "    bottom -= height\n",
    "    #bc = ax2.bar(0, height, width, bottom=bottom, color=bar_colors[j], label=label,\n",
    "                 #alpha=0.1 + 0.25 * j) #@|old. to remove\n",
    "    bc = ax2.bar(0, height, width, bottom=bottom, color=bar_colors[j], label=label) #@|updated!\n",
    "    ax2.bar_label(bc, labels=[f\"{height:0.0%}\"], label_type='center', fontsize = 17)\n",
    "    \n",
    "    labels_ordered.append(label) #this is for the tpf plot\n",
    "labels_ordered.reverse() #most contaminant sources first\n",
    "idx_other = np.where(np.array(labels_ordered) == 'Other')[0][0] #idxs of 'Other' to skip the color code in the next plot\n",
    "\n",
    "#ax1.set_title('Contaminant flux', fontsize = 14)\n",
    "#ax2.legend(bbox_to_anchor=(0.462,0.865), loc=\"upper left\",  bbox_transform=fig.transFigure, fontsize = 15)\n",
    "ax2.legend(bbox_to_anchor=(0.420,1.422), loc=\"upper left\", \\\n",
    "           bbox_transform=fig.transFigure, fontsize = 18, framealpha=0.8)\n",
    "ax2.axis('off')\n",
    "#ax2.set_xlim(- 2.5 * width, 2.5 * width)\n",
    "ax2.set_xlim(- 1 , 0.3)\n",
    "ax2.set_ylim(-0.05, 1.0)\n",
    "\n",
    "# use ConnectionPatch to draw lines between the two plots\n",
    "theta1, theta2 = wedges[0].theta1, wedges[0].theta2\n",
    "center, r = wedges[0].center, wedges[0].r\n",
    "bar_height = sum(bar_ratios)\n",
    "\n",
    "# draw top connecting line\n",
    "x = r * np.cos(np.pi / 180 * theta2) + center[0]\n",
    "y = r * np.sin(np.pi / 180 * theta2) + center[1]\n",
    "con = ConnectionPatch(xyA=(-width / 2, bar_height), coordsA=ax2.transData,\n",
    "                      xyB=(x, y), coordsB=ax1.transData)\n",
    "con.set_color('k')\n",
    "con.set_linewidth(2)\n",
    "ax2.add_artist(con)\n",
    "\n",
    "# draw bottom connecting line\n",
    "x = r * np.cos(np.pi / 180 * theta1) + center[0]\n",
    "y = r * np.sin(np.pi / 180 * theta1) + center[1]\n",
    "con = ConnectionPatch(xyA=(-width / 2, 0), coordsA=ax2.transData,\n",
    "                      xyB=(x, y), coordsB=ax1.transData)\n",
    "con.set_color('k')\n",
    "ax2.add_artist(con)\n",
    "con.set_linewidth(2)\n",
    "\n",
    "\n",
    "#|----------------------\n",
    "#@|save the pie chart#@|\n",
    "#|----------------------\n",
    "\n",
    "if img_fmt == 'pdf' or img_fmt == 'pdfpng':\n",
    "    plt.savefig(f'output/{target_name}/{target_name}_S{sector}_piechart.pdf',\\\n",
    "                bbox_inches = 'tight', pad_inches = 0)\n",
    "if img_fmt == 'png'or img_fmt == 'pdfpng':\n",
    "    plt.savefig(f'output/{target_name}/{target_name}_S{sector}_piechart.png',\\\n",
    "                bbox_inches = 'tight', dpi = 400, pad_inches = 0)\n",
    "    \n",
    "#print('')\n",
    "print('\\033[1m' + f'Your pie chart {target_name}_S{sector}_piechart.pdf/png has been successfully generated and saved'+'\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915bda59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Generating the heatmap plot of {target_name} (Sector {sector}) ...')\n",
    "fig = plt.figure(figsize=(6.93, 5.5))\n",
    "_, ny, nx = np.shape(tpf)\n",
    "gs = gridspec.GridSpec(1,3, height_ratios=[1], width_ratios=[1,0.05,0.01])\n",
    "gs.update(left=0.05, right=0.95, bottom=0.12, top=0.95, wspace=0.01, hspace=0.03)\n",
    "ax1 = plt.subplot(gs[0,0])\n",
    "maskcolor = 'red'\n",
    "\n",
    "norm = ImageNormalize(stretch=stretching.LogStretch(), vmin = 0.1, vmax = 99) \n",
    "#norm = ImageNormalize(vmin = 0.1, vmax = 99)                                  \n",
    "\n",
    "splot = plt.imshow(CROWDSAP_pixel_by_pixel*100,\\\n",
    "                   zorder=0,alpha =1, \n",
    "          extent=[tpf.column-0.5,tpf.column+nx-0.5,tpf.row+ny-0.5,tpf.row-0.5], norm = norm, cmap = 'viridis')\n",
    "\n",
    "\n",
    "for i in range(aperture_mask.shape[0]):\n",
    "    for j in range(aperture_mask.shape[1]):\n",
    "        if aperture_mask[i, j]:\n",
    "            ax1.add_patch(patches.Rectangle((j+tpf.column-0.5, i+tpf.row-0.5),\n",
    "                                            1, 1, color=maskcolor, fill=True,alpha=0.2))\n",
    "            ax1.add_patch(patches.Rectangle((j+tpf.column-0.5, i+tpf.row-0.5), \n",
    "                                            1, 1, color=maskcolor, fill=False,alpha=1,lw=2))\n",
    "                    \n",
    "                \n",
    "if plot_percentages:                \n",
    "                    \n",
    "    for i in range(tpf.shape[1]):\n",
    "        for j in range(tpf.shape[2]):\n",
    "            \n",
    "            if np.round(CROWDSAP_pixel_by_pixel[i, j] * 100, 1) == 0.0:\n",
    "                continue\n",
    "\n",
    "            #@|trick to avoid 100.0 values (put instead 100)\n",
    "            if np.round(CROWDSAP_pixel_by_pixel[i, j] * 100, 1) == 100.0:\n",
    "                text = ax1.text(j+tpf.column, i+tpf.row, str(100),\n",
    "                           ha=\"center\", va=\"center\", color=\"k\", zorder = 1000, fontsize = 10.5) \n",
    "\n",
    "            else: \n",
    "                #text = ax1.text(j+tpf.column, i+tpf.row, np.round(CROWDSAP_pixel_by_pixel[i, j] * 100, 1),\n",
    "                               #ha=\"center\", va=\"center\", color=\"k\", zorder = 1000, fontsize = 10.5)\n",
    "                \n",
    "                text = ax1.text(j+tpf.column, i+tpf.row, np.round(CROWDSAP_pixel_by_pixel[i, j] * 100, 1),\n",
    "                               ha=\"center\", va=\"center\", color=\"k\", zorder = 1000, fontsize = 10.5)\n",
    "            \n",
    "\n",
    "#@|#####\n",
    "plt.xlabel('Pixel Column Number', fontsize=14, zorder=200)\n",
    "plt.ylabel('Pixel Row Number', fontsize=14, zorder=200)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "\n",
    "#@|##################################################@|\n",
    "#@|---Include the star locations within the plot----#@|\n",
    "#@|##################################################@|\n",
    "#@|Note. The circle sizes are scaled to the stellar fluxes (table['flux']). In particular, the 's' parameter\n",
    "#@|is proportional to table['flux'], so that the fluxes are proportinal to the total area.\n",
    "\n",
    "        \n",
    "#@|our target star\n",
    "if plot_target:\n",
    "    plt.scatter(pixel_coords[idx_target][0]+tpf.column, pixel_coords[idx_target][1]+tpf.row, \\\n",
    "                     s = (scale_factor*table['flux'][idx_target]), c = '#25BDB0', ec = 'k', lw = 1.5, \\\n",
    "                alpha = 0.8, zorder = 99, label = target_name)\n",
    "        \n",
    "\n",
    "#@|the N most contaminant sources\n",
    "if plot_main_contaminants:\n",
    "    heat_colors =  get_hex_color_list(num_colors = n_sources+1, saturation = 0.5, value = 1.0)    \n",
    "    heat_colors = heat_colors[::-1]\n",
    "    #heat_colors.pop(0)\n",
    "    heat_colors.pop(idx_other)\n",
    "    #for i,index in enumerate(idxs_selected_contaminant_sources):\n",
    "    for i,index in reversed(list(enumerate(idxs_selected_contaminant_sources))):\n",
    "        plt.scatter(pixel_coords_selected[i][0]+tpf.column, pixel_coords_selected[i][1]+tpf.row,\\\n",
    "                    s = (scale_factor*table['flux'][index]), c = heat_colors[i], alpha = 0.8, ec = 'k', \\\n",
    "                   zorder = 100000, label = bar_labels[i])      \n",
    "\n",
    "#@|all the remaining Gaia DR3 sources\n",
    "if plot_all_gaia:\n",
    "    for i in range(len(table)):\n",
    "        plt.scatter(pixel_coords[i][0]+tpf.column, pixel_coords[i][1]+tpf.row,\\\n",
    "                    s = (scale_factor*table['flux'][i]), c = 'lightgrey', ec = 'w', alpha = 0.3, zorder = 98)\n",
    "        \n",
    "    \n",
    "plt.ylim(tpf.row+ny-0.5, tpf.row-0.5)\n",
    "#plt.ylim(tpf.row+ny-0.5-1, tpf.row-0.5)       #Arrange for the 13x11 TPF, larger than the PRF arrays\n",
    "plt.xlim(tpf.column-0.5, tpf.column+nx-0.5)\n",
    "\n",
    "\n",
    "#@|to fix marker sizes for the legend\n",
    "#@|see https://stackoverflow.com/questions/24706125/setting-a-fixed-size-for-points-in-legend\n",
    "legend_marker_size = 60\n",
    "def updatescatter(handle, orig):\n",
    "    handle.update_from(orig)\n",
    "    handle.set_sizes([legend_marker_size])\n",
    "plt.legend(loc = loc_legend, handler_map={PathCollection : HandlerPathCollection(update_func=updatescatter)},\\\n",
    "           framealpha=0.8, fontsize = 12).set_zorder(10000)\n",
    "\n",
    "#@|--------\n",
    "#@|COLORBAR\n",
    "#@|--------\n",
    "cbax = plt.subplot(gs[0,1]) # Place it where it should be.\n",
    "pos1 = cbax.get_position() # get the original position\n",
    "pos2 = [pos1.x0 - 0.075, pos1.y0, pos1.width, pos1.height]\n",
    "cbax.set_position(pos2) # set a new position\n",
    "\n",
    "#cbar_ticks = np.array([10, 20, 40, 60, 80])\n",
    "#cb = Colorbar(ax = cbax, mappable = splot, orientation = 'vertical',\n",
    "              #ticklocation = 'right', ticks =  cbar_ticks)\n",
    "\n",
    "cb = Colorbar(ax = cbax, mappable = splot, orientation = 'vertical',\n",
    "              ticklocation = 'right')\n",
    "\n",
    "if plot_target_name:\n",
    "    cb.set_label(f'Flux ratio from {target_name} (%)', labelpad=10, fontsize=14)\n",
    "else:\n",
    "    cb.set_label('Flux ratio from the target star (%)', labelpad=10, fontsize=14)\n",
    "\n",
    "#@|---------------------------\n",
    "#@|----save the heatmap-----#@|\n",
    "#@|---------------------------\n",
    "\n",
    "\n",
    "if img_fmt == 'pdf' or img_fmt == 'pdfpng':\n",
    "    plt.savefig(f'output/{target_name}/{target_name}_S{sector}_heatmap.pdf', \\\n",
    "                bbox_inches = 'tight', pad_inches = 0)\n",
    "if img_fmt == 'png' or img_fmt == 'pdfpng':\n",
    "    plt.savefig(f'output/{target_name}/{target_name}_S{sector}_heatmap.png', \\\n",
    "                bbox_inches = 'tight', pad_inches = 0, dpi = 400)\n",
    "    \n",
    "#print('')\n",
    "print('\\033[1m' + f'Your heatmap {target_name}_S{sector}_heatmap.pdf/png has been successfully generated and saved'+'\\033[0m')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfba4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589d0241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#@|save the list of the 'n_sources' most contaminant sources\n",
    "#@|++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee5a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|we obtain the TIC IDs (tic_names_selected) of the n_sources from the Gaia DR2 IDS\n",
    "#@|----This functionality is deprecated due to the limited number of TICs found-----\n",
    "\n",
    "#tic_names_selected = []\n",
    "#for i in range(n_sources):\n",
    "    \n",
    "    #try:\n",
    "        #cat = Catalogs.query_object(f'Gaia_DR3_{gaia_names_selected[i]}', catalog=\"TIC\")\n",
    "        #idx_cat = np.where(cat['GAIA'].value.data == str(gaia_names_selected[i]))[0][0]\n",
    "        #tic_names_selected.append(cat['ID'][idx_cat])\n",
    "        \n",
    "    #except:\n",
    "        ##print(f'No TIC number found for Gaia_DR3_{gaia_names_selected[i]} ')\n",
    "        #tic_names_selected.append('No TIC ID found')\n",
    "        #continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb41ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b9eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'output/{target_name}/{target_name}_S{sector}_contaminant_sources.dat', 'w+')\n",
    "\n",
    "header = ['Star,Gaia_ID,total_cont(%),rel_cont(%) \\n']\n",
    "f.writelines(header)\n",
    "\n",
    "for i in range(n_sources):\n",
    "    \n",
    "    crowdsap_sorted_ac = crowdsap_sorted[-n_sources:][::-1][i] * 100 #(in %)\n",
    "    total_cont_ac = relative_contam_sorted[:n_sources][i] * 100 #(in %)\n",
    "    \n",
    "    L = [str(i+1)+','+str(gaia_names_selected[i])+','+str(round(crowdsap_sorted_ac, 4))\\\n",
    "         +','+str(round(total_cont_ac, 4))+'\\n']\n",
    "    f.writelines(L)\n",
    "    \n",
    "#####################---rep---###############   \n",
    "\n",
    "f = open(f'output/{target_name}/{target_name}_S{sector}_contaminant_sources.dat', 'w+')\n",
    "\n",
    "header = ['Star,Gaia_ID,total_cont(%),rel_cont(%) \\n']\n",
    "f.writelines(header)\n",
    "\n",
    "for i in range(n_sources):\n",
    "    \n",
    "    crowdsap_sorted_ac = crowdsap_sorted[-n_sources:][::-1][i] * 100 #(in %)\n",
    "    total_cont_ac = relative_contam_sorted[:n_sources][i] * 100 #(in %)\n",
    "    \n",
    "    L = [str(i+1)+','+str(gaia_names_selected[i])+','+str(round(crowdsap_sorted_ac, 4))\\\n",
    "         +','+str(round(total_cont_ac, 4))+'\\n']\n",
    "    f.writelines(L)\n",
    "    \n",
    "\n",
    "print('\\033[1m' + f'The list of the {n_sources} most contaminant sources has been saved in {target_name}_S{sector}_contaminant_sources.dat'+'\\033[0m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c97798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@|----------------------\n",
    "#@|TRANSIT DEPTH ANALYSIS\n",
    "#@|----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1fd7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if transit_depth_analysis:\n",
    "    \n",
    "    if td_unit == 'ppm':\n",
    "        td = td / 1e6 #transit depth (0-1)\n",
    "    if td_unit == 'ppt':\n",
    "        td = td / 1000 #transit depth (0-1)\n",
    "    if td_unit == 'per':\n",
    "        td = td / 100 #transit depth (0-1)\n",
    "        \n",
    "    #@|transit depth uncorrected (e.g. SPOC td obtained from PDCSAP photometry)\n",
    "    if dilution_corr == True:\n",
    "        td = td * CROWDSAP\n",
    "        \n",
    "    f = open(f'output/{target_name}/{target_name}_S{sector}_undituled_transit_depths.dat', 'w+')\n",
    "\n",
    "    #if td_unit == 'ppm':\n",
    "        #header = ['Gaia_ID,TIC_ID,transit_depth(ppm) \\n']\n",
    "    #if td_unit == 'ppt':\n",
    "        #header = ['Gaia_ID,TIC_ID,transit_depth(ppt) \\n']\n",
    "    #if td_unit == 'per':\n",
    "        #header = ['Gaia_ID,TIC_ID,transit_depth(%) \\n']\n",
    "    #if td_unit == 'frac':\n",
    "        #header = ['Gaia_ID,TIC_ID,transit_depth \\n']\n",
    "        \n",
    "    header = ['Star,Gaia_ID,transit_depth(%) \\n']\n",
    "\n",
    "    f.writelines(header)\n",
    "\n",
    "    for i in range(n_sources):\n",
    "\n",
    "        #if td_unit == 'ppm':\n",
    "            #td_undiluted = td / crowdsap_sorted[-n_sources:][::-1][i] * 1e6\n",
    "\n",
    "        #if td_unit == 'ppt':\n",
    "            #td_undiluted = td / crowdsap_sorted[-n_sources:][::-1][i] * 1000\n",
    "\n",
    "        #if td_unit == 'per':\n",
    "            #td_undiluted = td / crowdsap_sorted[-n_sources:][::-1][i] * 100\n",
    "\n",
    "        #if td_unit == 'frac':\n",
    "            #td_undiluted = td / crowdsap_sorted[-n_sources:][::-1][i]\n",
    "            \n",
    "        td_undiluted = td / crowdsap_sorted[-n_sources:][::-1][i]\n",
    "\n",
    "        L = [str(i+1)+','+str(gaia_names_selected[i])+','+str(td_undiluted*100)+'\\n']\n",
    "        f.writelines(L)\n",
    "    \n",
    "    \n",
    "    #--------------------writting x2---------------------------\n",
    "    \n",
    "    f = open(f'output/{target_name}/{target_name}_S{sector}_undituled_transit_depths.dat', 'w+')\n",
    "\n",
    "    #if td_unit == 'ppm':\n",
    "        #header = ['Gaia_ID,TIC_ID,transit_depth(ppm) \\n']\n",
    "    #if td_unit == 'ppt':\n",
    "        #header = ['Gaia_ID,TIC_ID,transit_depth(ppt) \\n']\n",
    "    #if td_unit == 'per':\n",
    "        #header = ['Gaia_ID,TIC_ID,transit_depth(%) \\n']\n",
    "    #if td_unit == 'frac':\n",
    "        #header = ['Gaia_ID,TIC_ID,transit_depth \\n']\n",
    "        \n",
    "    header = ['Star,Gaia_ID,transit_depth(%) \\n']\n",
    "\n",
    "    f.writelines(header)\n",
    "\n",
    "    for i in range(n_sources):\n",
    "\n",
    "        #if td_unit == 'ppm':\n",
    "            #td_undiluted = td / crowdsap_sorted[-n_sources:][::-1][i] * 1e6\n",
    "\n",
    "        #if td_unit == 'ppt':\n",
    "            #td_undiluted = td / crowdsap_sorted[-n_sources:][::-1][i] * 1000\n",
    "\n",
    "        #if td_unit == 'per':\n",
    "            #td_undiluted = td / crowdsap_sorted[-n_sources:][::-1][i] * 100\n",
    "\n",
    "        #if td_unit == 'frac':\n",
    "            #td_undiluted = td / crowdsap_sorted[-n_sources:][::-1][i]\n",
    "\n",
    "        td_undiluted = td / crowdsap_sorted[-n_sources:][::-1][i]\n",
    "        \n",
    "        L = [str(i+1)+','+str(gaia_names_selected[i])+','+str(td_undiluted*100)+'\\n']\n",
    "        f.writelines(L)\n",
    "        \n",
    "        \n",
    "    print('\\033[1m' + f'The transit depth analysis has been saved in {target_name}_S{sector}_undituled_transit_depths.dat ')\n",
    "    print('\\033[0m')\n",
    "    print('     ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('      Thank you for using TESS-cont :-D We hope to see you again soon!')\n",
    "    print('     ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('')\n",
    "        \n",
    "        \n",
    "else:\n",
    "    \n",
    "    print('')\n",
    "    print('     ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('      Thank you for using TESS-cont :-D We hope to see you again soon!')\n",
    "    print('     ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb70d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9199db0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
